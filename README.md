# Multi-Document RAG Chatbot (Local LLM)

A production-ready **Retrieval-Augmented Generation (RAG)** chatbot that allows users to upload multiple PDFs and ask contextual questions using a **local LLM (Ollama + LLaMA 3)**.

This project demonstrates how to build a **fully offline RAG system** without using paid APIs like OpenAI.

---

## ğŸš€ Features

- Upload and process **multiple PDF documents**
- Ask questions across all uploaded documents
- Semantic search using **ChromaDB**
- Local embeddings using **nomic-embed-text**
- Local LLM inference using **Ollama (LLaMA 3)**
- Interactive **Streamlit Chat UI**
- No internet or paid API required

---

## ğŸ§  Tech Stack

- Python
- LangChain
- Ollama
- LLaMA 3
- ChromaDB
- Streamlit

---

## ğŸ“ Project Structure

llm-rag-system/ â”‚ â”œâ”€â”€ app.py       
# Streamlit UI â”œâ”€â”€ ingest.py      
# PDF ingestion logic â”œâ”€â”€ query.py       
# Question answering â”œâ”€â”€ rag_ollama.py   
# LLM + retriever setup â”œâ”€â”€ data/       
# Uploaded PDFs â”œâ”€â”€ db/            
# Vector database â””â”€â”€ README.md

---

## âš™ï¸ How to Run the Project

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt

 ollama pull llama3
 python ingest.py
 streamlit run app.py
 
 ğŸ‘¨â€ğŸ’» Author
Pradip Verma
B.Tech AI & Data Science
Interested in LLMs, RAG systems & AI products
